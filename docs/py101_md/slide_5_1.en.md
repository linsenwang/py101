# Chapter 5: Machine Learning

!!! info "What You Will Learn"
    *   The general idea of machine learning.
    *   Key terminologies used in the machine learning context.
    *   Several examples using the `scikit-learn` module.

## What Machine Learning Is About

Recall the Cobb-Douglas production function example, where we aimed to find the "best" `𝛼` that gives us the closest estimation of the output for given capital and labor. This is a foundational concept in understanding how machine learning models are developed and refined.

### Key Terminology

Understanding the following terms is crucial in machine learning:

*   **Model**: The equation describing the relationship between variables.
    *   For example, `𝑌=𝐾𝛼𝐿1−𝛼` describes the relationship between `𝑌`, `𝐾`, and `𝐿`.
*   **Target variable**: The variable we want to predict (on the left-hand side) or to know how it is generated.
    *   Also known as the dependent or response variable in econometrics.
*   **Predictors**: Variables used to explain or predict the target variable (on the right-hand side).
    *   Also known as independent or explanatory variables (or covariates) in econometrics.
*   **Parameters**: Symbols whose values are unknown in the model.
    *   For example, `𝛼` in the Cobb-Douglas function.
    *   Once we "know" the parameter values, we can compute the model-implied target variable.
    *   Functional models and their parameters represent our understanding of the world that generates the data.
*   **Estimates**: Because true parameters are unknown, we need to guess their best values (as we did to find `𝛼`).
    *   Even if we find a "best" value, we don't know for certain if the data is actually generated by our chosen model. It's possible our understanding of the world (the model) is not fully correct.
    *   In such cases, parameters from an incorrect model generally cannot be truly "discovered." The best we can achieve for a given model is an **estimate**.

!!! question "Exercise: Identifying Model Components"
    What are the model, variables, and parameters in the following scenarios?  
    
    1.  A research team wants to predict the house price (`𝑃`) for some houses. They collected the area (`𝐴`), the distance to the town center (`𝐷`), the age (`𝐺`), and the materials (`𝑀`) of several houses. They decided to use a complicated relationship:
        `𝑃=𝑓(𝑙(𝐴,𝐷,𝐺,𝑀))`
        where `𝑙(𝐴,𝐷,𝐺,𝑀)=𝑎+𝑏1𝐴+𝑏2𝐷+𝑏3𝐺+𝑏4𝑀`,
        and `𝑓(𝑥)=𝑥` only when `𝑥≥0`; `0` otherwise.
    
    2.  Another team wanted to learn if bond prices (`𝑅`) will go up or down. They collected several variables, but due to privacy issues, they only tell us these are `𝑋1,𝑋2,…,𝑋15`. Their way of prediction is:
        `Prob(𝑅>0) = exp(𝑙(𝑋1,𝑋2,…,𝑋15)) / (1+exp(𝑙(𝑋1,𝑋2,…,𝑋15)))`

### Classification vs. Regression

Machine learning tasks can be broadly categorized based on the nature of the target variable:

*   **Classification**: This task involves predicting discretized target variables.
    *   Examples:
        *   Will bond prices go up or down (instead of how much they will move)?
        *   Will a student pass a course (instead of their exact score)?
        *   Will Trump increase tariffs (instead of the rate by which they are increased)?
*   **Regression**: This task involves predicting the exact value of a continuous target variable.

### Loss Function

A **loss function** is a rule that tells the machine how well the parameters are estimated. It quantifies the difference between the predicted values and the actual values.

*   For example, we have seen the **absolute-error loss**:
    `𝐿(𝒚,ො𝒚) = ෍𝑖 |𝑦𝑖 − ො𝑦𝑖|`
*   Another commonly used error is the **squared-error loss**:
    `𝐿(𝒚,ො𝒚) = ෍𝑖 (𝑦𝑖 − ො𝑦𝑖)²`

We can take derivatives of the loss function with respect to the parameters to find the estimates. This process is also known as "fitting the model to data."

!!! note "Fitting the Model"
    The parameters are implicitly part of the prediction `ො𝑦𝑖`, which is generated by the model using those parameters and the input predictors `𝒙𝒊`.

Machine learning methods are techniques that fit models to data and help find the best data-generating mechanism (represented by parameter estimates) from a class of functions (i.e., functions with the same form but different parameters).

The problem we often want to solve can be simplified and summarized as finding:
`ො𝑦𝑖 = 𝑓(𝒙𝒊; ෡𝜽)`
where `෡𝜽` represents the estimated parameters. In Python, we aim to find such a function `𝑓`.

## The Slope-Interception Example

In Chapter 2.3, we encountered an example of a function that creates another function with a fixed slope and intercept.

```python
def intercept_1():
    a = 1
    def slope_2(x):
        return 2*x + a
    return slope_2

linear_trans = intercept_1()
linear_trans(3)
```

We then had an exercise to modify this function to be more general, allowing arbitrary slope and intercept values:

```python
def intercept_and_slope(a, b):
    # a is the intercept
    def evaluation(x):
        # b is the slope
        return a + b*x  # a and b are nonlocals
    return evaluation
```

Now, let's extend this to fit some data. Consider the following dataset:

| No. | y | x |
|-----|---|---|
| 0   | 0 | 0 |
| 1   | 0 | 1 |
| 2   | 1 | 2 |
| 3   | 3 | 3 |
| 4   | 5 | 4 |

!!! question "Exercise: Fitting a Linear Model"
    1.  Please create a scatter plot to visualize how the data looks.  
    2.  Make use of the `intercept_and_slope` function we wrote, or define new functions, to find the best parameters `a` (intercept) and `b` (slope) for the linear model `𝑦 = 𝑎 + 𝑏𝑥` that fits the provided data.  

## Two Classic Machine Learning Methods

You may already feel the difficulty of writing your own code to find the estimates for even the simplest machine learning method (linear regression). For some other...
# Understanding Complex Machine Learning Models and Scikit-learn

For some other machine learning methods beyond linear regression, the function forms are much more complicated, and we sometimes only use pictures to represent them. For example, the neural network model and the classification-and-regression tree model.

## Two Classic Machine Learning Methods

### Neural Network Model Example

Suppose we have 5 observations and 3 explanatory variables. We want to explain one continuous variable `y`. A neural network model can be visualized.

A simple representation:
Inputs `x1`, `x2`, `x3` are processed by a neuron `z1[1]` using a linear function with parameters `a, b1, b2, b3`. The output of this neuron then goes through a ReLU activation function to produce `c1[1]`, which contributes to the final output `f`.

Let’s make the neural network more concrete and complex.
Given inputs `x1`, `x2`, `x3`:
The activations `â1` and `â2` for two neurons in a layer could be calculated as:

`â1 = b1<1> + w11<1>x1 + w12<1>x2 + w13<1>x3`
`â2 = b2<1> + w21<1>x1 + w22<1>x2 + w23<1>x3`

These activations `a1`, `a2` would then be passed through activation functions and potentially further layers to produce the final prediction `ŷ`.

### Tree Model Example

Let’s review the iris data, where flowers are classified into three types. If we only focus on sepal width and sepal length, we could plot these features for a few flowers.

A tree model uses horizontal and vertical lines to split the feature space (the x-space). We can continue this splitting process until we achieve the desired number of sub-spaces, each corresponding to a specific prediction or class.

!!! question "Exercise: Analyzing Neural Network and Tree Models"
    1.  Summarize the model structure and parameters in the previous neural network and tree examples.
    2.  Can you write Python code to implement the functions described for these two models?
    3.  For these models, can you take the derivative of the prediction function with respect to the parameters to help find the optimal parameter estimates?

## The Scikit-learn Module

When model structures are relatively simple, we can write our own functions to represent the model and find the derivatives mathematically for optimization. However, when models become more complex, it is challenging to define the functions and compute derivatives manually.

!!! info "Complex Neural Networks"
    A typical neural network model may contain thousands of cells (neurons) and multiple layers. The complexity can be significant.
    (Based on an idea from a picture modified from: `https://doc.comsol.com/6.2/doc/com.comsol.help.comsol/comsol_ref_definitions.19.050.html`)

In such cases, we turn to the `scikit-learn` module for its robust implementations of various machine learning algorithms.

!!! warning "Installation Note"
    To install scikit-learn, it is recommended to use conda. Avoid installing `sklearn` via pip if you are using Anaconda, as `scikit-learn` is the correct package name.
    ```bash
    conda install scikit-learn
    ```

The machine learning methods in `scikit-learn` follow a very similar API. Let’s start with an example of a tree model to classify the iris flowers.

### Tree Model with Scikit-learn

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets

# Load the iris dataset
iris = datasets.load_iris()

# Initialize the Decision Tree Classifier
clf = DecisionTreeClassifier()

# Fit the model (using all but the last sample for training here as an example)
clf = clf.fit(iris.data[:-1], iris.target[:-1])

# Predict on the training data (excluding the last sample)
res = clf.predict(iris.data[:-1])
```

### Neural Network with Scikit-learn

Let’s try a neural network for a regression task using `scikit-learn`.

```python
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Generate a synthetic regression dataset
X, y = make_regression(n_samples=200, n_features=20, random_state=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Initialize the Multi-layer Perceptron Regressor
regr = MLPRegressor(random_state=1, max_iter=2000, tol=0.1)

# Fit the model
regr.fit(X_train, y_train)

# Predict on the first two samples of the test set
predictions = regr.predict(X_test[:2])

# Score the model on the test set
score = regr.score(X_test, y_test)
```

### Linear Model with Scikit-learn

Of course, `scikit-learn` also provides simple linear models:

```python
from sklearn import linear_model

# Initialize the Linear Regression model
reg = linear_model.LinearRegression()

# Fit the model
reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])

# Get the coefficients
coefficients = reg.coef_
```

!!! info "Further Learning Resources"
    Scikit-learn offers a wide array of other machine learning models. If you need more references:
    *   "Python Machine Learning" by Sebastian Raschka and Vahid Mirjalili is a great starting point.
    *   Online courses and videos (e.g., from Coursera, fast.ai) can offer a great focus on neural networks.
    *   For more advanced versions of neural networks, consider exploring libraries like TensorFlow and PyTorch.

## Model Selection and Cross-Validation

In neural network modeling, we can design our own network architectures. For example, we have seen a simple network with only one layer and one cell, and we can also envision more complex networks with multiple layers and many cells in each layer.

A key question arises: if both simple and complex models are types of neural network models, which one is better for a given task? To answer this, we need to select a specific model configuration. This process is known as the **model selection problem**.

### Cross-Validation

**Cross-validation** is a technique that helps us select the best model by evaluating its performance on different subsets of the data, providing a more robust estimate of its generalization ability.

!!! quote "Insight on Cross-Validation"
    The core idea of cross-validation is to split the training data into multiple folds. The model is trained on some folds and validated on the remaining fold. This process is repeated, and the performance scores are averaged.
    (For visual explanations, refer to the scikit-learn documentation on cross-validation: `https://scikit-learn.org/stable/modules/cross_validation.html`)

!!! question "Exercise: Linear Model Training and Prediction"
    Let’s consider a simple linear model with 10 explanatory variables.
    The data are stored in `sparsedata.csv`. This dataset (presumably 70 observations) already includes the target variable. If we were only interested in these 70 observations, we might not strictly need a predictive model for them.
    However, later on, we collected another 30 observations for which the target variable is unknown. These new explanatory variables are stored in `X_test.csv`.
    Your task will be to train a model on `sparsedata.csv` to help predict the target for the observations in `X_test.csv`.
    *(Further instructions for this exercise would typically follow, detailing steps for loading data, training the model, and making predictions.)*
# Model Refinement and Kaggle Overview

Continuing from our discussion on loading data and basic model training, we now focus on the crucial step of model selection and evaluation. Once features are prepared (e.g., in `X_test.csv`) and a general modeling approach is chosen (like linear models with scikit-learn), we often need to decide on the best specific model structure or parameters.

## Model Selection and Cross-Validation

Model selection involves choosing the best model from a set of candidate models. Cross-validation is a robust technique used to estimate the performance of machine learning models and to prevent overfitting by testing the model on different subsets of the training data.

!!! question "Exercise: Model Selection Using Cross-Validation"
    In this exercise, we'll explore selecting the best model from a predefined set using cross-validation. Assume that not all explanatory variables contribute meaningfully to predicting the target simultaneously. Therefore, we will evaluate the following candidate models:

    *   $y = a + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + b_5x_5$
    *   $y = a + b_1x_1 + b_3x_3 + b_5x_5 + b_7x_7 + b_9x_9$
    *   $y = a + b_2x_2 + b_4x_4 + b_6x_6 + b_8x_8 + b_{10}x_{10}$
    *   $y = a + b_1x_1 + b_4x_4 + b_8x_8$
    *   $y = a + b_2x_2 + b_6x_6 + b_9x_9$

    Your task is to use cross-validation techniques to select the most suitable model from the list above based on its performance.

### Evaluating Performance on Test Data

After selecting a model using cross-validation, it's essential to evaluate its performance on unseen data. For this purpose, the target variables for the test dataset have been collected and stored in `target_test.csv`.

Use your selected model to make predictions on the 30 observations in the test set and assess how well your model performs.

## A Practical View of Kaggle

Kaggle is a popular platform for data science competitions, datasets, and learning resources. Let’s explore the Optiver Realized Volatility Prediction competition as an example to understand the basic layout and information available on Kaggle.

Navigating a typical Kaggle competition involves understanding several main tabs:

### Key Sections of a Kaggle Competition Page

#### Overview Tab
The **Overview** tab provides the background context for the competition. This includes:

*   A detailed **Description** of the problem.
*   The **Evaluation** metric and rules that determine how submissions are scored.
*   The competition **Timeline**, which is crucial for active participation, detailing start dates, end dates, and any phased deadlines.

#### Data Tab
The **Data** tab is where Kaggle provides the datasets required for training and testing your machine learning models.
*   You'll find descriptions of the data files, their formats, and often a data dictionary explaining the variables.
*   Download links for the datasets are also located here.

!!! info "Handling Diverse Data Formats"
    While CSV files are common, datasets on Kaggle can be stored in various other formats (e.g., Parquet, JSON, HDF5). In such cases, you'll need to learn how to import and process these data formats in Python using appropriate libraries.

Further scrolling down this tab usually reveals detailed file descriptions and download links.

#### Code Tab
The **Code** tab (often called "Notebooks" or "Kernels") showcases public code shared by other participants.
*   This is an excellent resource for learning new techniques, understanding data preprocessing steps, or seeing how others approach the modeling problem.
*   Exploring these notebooks can significantly accelerate your learning in Python and machine learning.

!!! warning "Ethical Code Usage"
    While learning from others' code is encouraged, it is crucial to respect intellectual property. If you use or adapt code from public notebooks, you must clearly acknowledge the original author and source in your own work.

### Learning from the Community

Kaggle fosters a strong community spirit. Beyond code, discussions forums associated with competitions and datasets are valuable for asking questions, sharing insights, and collaborating.

!!! quote "Feed me DATA!"
    Source: [Jamie Sale Cartoonist](https://www.jamiesale-cartoonist.com/free-cartoon-robot-vector/)
