# Chapter 7: LLMs and RAG

## What You Will Learn

This chapter will guide you through several key aspects of working with Large Language Models (LLMs):

*   Converse with LLMs directly from Python.
*   Understand some very basic mechanisms of how LLMs operate.
*   Learn about the RAG (Retrieval Augmented Generation) technique.
*   Explore some practical examples of RAG.

## Calling LLM APIs

We are all familiar with conversing with Large Language Models (LLMs) on their websites through dialogue boxes. While convenient, this method can be restrictive, especially when dealing with sensitive information or requiring repetitive tasks.

For instance, imagine you're editing a statement of purpose for a university application. This document likely contains personal information you wouldn't want an LLM to retain or infer patterns from. If you need several rounds of discussion with the LLM to refine your statement for clarity and impact, you'd have to manually replace sensitive details like names with placeholders in each interaction.

Using Python to call LLM APIs offers a more streamlined and secure approach. You can define variables for sensitive data and pass a modified, privacy-preserving version of your text to the model.

```python
name = "Tony"
message = "Hello, {name}!"
# print(message.format(name=name))
```

### Environment Setup

To begin, let's prepare our Python environment. You'll need to install the following modules:

*   `langchain`: A framework that simplifies calling LLMs from Python.
*   `langchain-deepseek`: A specific integration for DeepSeek APIs.
*   `textwrap`: A utility for formatting output neatly.
*   `dotenv`: A module to securely manage API keys by storing them in a `.env` file.

!!! info "DeepSeek API Key"
    You will need a DeepSeek API key.
    1. Create a DeepSeek account.
    2. Log in to [https://platform.deepseek.com/usage](https://platform.deepseek.com/usage) to generate an API key.
    3. For this class, you may use a provided key. After the class, if you wish to continue practicing, you can add funds to your account (which may be eligible for reimbursement for course-related practice).

### Example: Talking to DeepSeek from Python

Let's see how to communicate with the DeepSeek LLM using Python.

```python
from langchain_deepseek import ChatDeepSeek
import textwrap

# For more details, refer to the Langchain documentation:
# https://python.langchain.com/docs/integrations/chat/deepseek/

llm = ChatDeepSeek(model="deepseek-chat")

messages = [
    ("system", "You are a helpful editor. Help me polish the application."),
    ("human", "My name is {myname}. I’m writing to apply to your university."),
]

# Note: To run this, you would replace {myname} or format it appropriately.
# For simplicity in this example, we'll assume it's a placeholder.
# A more complete example would be:
# my_name_value = "Your Actual Name"
# messages = [
#     ("system", "You are a helpful editor. Help me polish the application."),
#     ("human", f"My name is {my_name_value}. I’m writing to apply to your university."),
# ]


resp_msg = llm.invoke(messages)

# To display the response content:
# print(textwrap.fill(resp_msg.content, width=70))
```

**Sample Model Output:**

If you were to run the code (after setting up your API key and properly formatting the `myname` variable), you might receive a response similar to this:

```
Here’s a polished and professional version of your application opening:
---
**Subject:** Application for Admission

Dear [Admissions Committee/Recipient’s Name],

My name is [Your Name], and I am writing to express my sincere interest in
applying to [University Name] for [program name, if applicable].
[Optional: Add 1–2 sentences about why you’re drawn to the university or
program—e.g., "I am..."]
```

## Conversation with LLMs

A true conversation differs from a one-off question-and-answer exchange because the LLM needs to "remember" the context of the preceding dialogue. To achieve this, we must create a continuous flow of interaction and have a system for storing the conversation history.

### Message Types

When we communicate with LLMs, our messages convey different types of information, as seen in the previous example:

*   **System Message**: Provides background knowledge, instructions, or a persona for the LLM (e.g., "You are a helpful editor.").
*   **Human Message**: Contains our actual questions, prompts, or inputs to the LLM (e.g., "My name is {myname}...").
*   **AI Message**: Represents the response generated by the LLM.

We will store all this information, typically in a list, to maintain the conversational thread.

### Storing Conversation History

Let's initialize a list to store our chat history:

```python
# (Assuming HumanMessage and AIMessage are imported from langchain.schema)
# from langchain.schema import HumanMessage, AIMessage
chat_history = []
```

!!! question "Exercise: Handling Open-Ended Conversations"
    If we are not sure when the conversation will end (e.g., the user can type "quit" to exit), how can we structure the Python code to handle this continuous interaction?

    This typically involves a loop that continues until a specific exit condition is met.

### Implementing a Multi-Round Discussion

Here's how you can set up a loop for an ongoing conversation with an LLM:

```python
# (Ensure llm is defined, e.g., from the previous DeepSeek example)
# from langchain_deepseek import ChatDeepSeek
# from langchain.schema import HumanMessage, AIMessage
# llm = ChatDeepSeek(model="deepseek-chat")
# chat_history = [
#     ("system", "You are a helpful conversational AI.") # Initial system message if needed
# ]
# Or using HumanMessage/AIMessage objects if you imported them
# from langchain.schema import SystemMessage
# chat_history = [
#    SystemMessage(content="You are a helpful conversational AI.")
# ]


while True:
    query = input("You: ")
    if query.lower() == "quit":
        break
    
    # Add user's message to history
    # If using tuples:
    chat_history.append(("human", query))
    # If using Message objects:
    # chat_history.append(HumanMessage(content=query))
    
    # Get LLM's response
    result = llm.invoke(chat_history)
    response = result.content
    
    # Add LLM's response to history
    # If using tuples:
    chat_history.append(("ai", response))
    # If using Message objects:
    # chat_history.append(AIMessage(content=response))
    
    print(f"AI: {response}")

```

!!! note "Message Objects"
    In `langchain`, messages are often represented by specific classes like `SystemMessage`, `HumanMessage`, and `AIMessage` imported from `langchain.schema` or `langchain_core.messages`. The example above uses tuples `("type", "content")` for simplicity, which `ChatDeepSeek` and other Langchain chat models can often interpret directly. However, using the explicit message objects is a more robust practice.

## What LLMs Are About

Based on our experiences, LLMs primarily act as sophisticated chatbots. But how do they manage to "understand" and process human language?

!!! question "Exercise: LLM Language Processing Steps"
    In Chapter 2, we learned how pictures are stored in a computer (as pixel data). In Chapter 6, we saw how numeric covariates are used to predict responses in statistical models. Considering this, what steps do you think LLMs take to analyze human languages?

    Think about how text, which is symbolic, can be converted into a format that a mathematical model can process.

### The Recursive Neural Network (RNN) Structure

One of the foundational architectures that enabled advancements in processing sequential data like text is the Recursive Neural Network (RNN), and its more advanced variants like LSTMs (Long Short-Term Memory networks).

!!! info "Visualizing RNNs/LSTMs"
    The structure of these networks is designed to handle sequences by maintaining a state or "memory" of previous elements in the sequence.
    *Source for further reading and diagrams: [Understanding LSTMs by Christopher Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)*

    A key idea is that the network's output at a given step depends not only on the current input but also on what it has processed previously.

### The Concept of Embedding

The most crucial concept for understanding how LLMs process language is **embedding**. Embedding is the process of converting textual units (words, phrases, sentences, paragraphs, or even entire documents) into numerical representations, specifically vectors in a high-dimensional space.

These vectors capture the semantic meaning and context of the text, such that similar words or phrases have similar vector representations. This numerical format is what allows neural networks to perform mathematical operations on language.

!!! quote "Insight: Embedding is Key"
    Embedding is the bridge that allows computational models to "understand" and manipulate the nuances of human language by translating it into the language of mathematics (vectors).

    *Further context on embeddings can also be found in resources like the previously mentioned article by Christopher Olah.*

!!! question "Exercise: The Core Task of LLMs"
    Technically, what are LLMs doing when they generate text or answer questions? What is their fundamental objective at a high level?

    Consider the input they receive (often a prompt and context) and the output they produce.
```
# Advanced Topics in AI: LLMs, RAG, and Application Recall

## Understanding Large Language Models (LLMs)

### Technically, What Are LLMs Doing?
Large Language Models (LLMs) are sophisticated neural networks designed to process and generate human language. They function by taking embedded language inputs (text converted into numerical representations) and producing responses. The nature and quality of these responses are heavily influenced by the input messages, often referred to as prompts.

!!! warning "A Challenge with LLMs: Hallucinations"
    A common issue with general-purpose LLMs is their tendency to "hallucinate," meaning they may generate incorrect, nonsensical, or fabricated information. A well-known example of this is the generation of non-existent academic references.

To address such limitations and enhance the reliability of LLM outputs, techniques like Retrieval-Augmented Generation (RAG) have been introduced.

## Enhancing LLM Responses with Retrieval-Augmented Generation (RAG)

RAG is a technique designed to provide LLMs with more relevant and factual information when responding to queries. By augmenting the input prompt with information retrieved from a trusted knowledge base, RAG aims to ground the LLM's responses in verifiable data, thereby reducing hallucinations and improving accuracy.

### The RAG Prompt Structure

The core idea of RAG is to modify the user's prompt to include relevant documents or pieces of information. A generalized structure for such a prompt might look like this:

```text
combined_input = (
    "Here are some documents that might help answer the question: "
    + doc_info
    + "\n\nRelevant Documents: \n"
    + "\n\n"
    + "\n".join([doc for doc in relevant_docs]) # Placeholder for actual document joining
    + "\n\nPlease provide an answer based only on the provided documents. If the "
    + "answer is not found in the documents, respond with 'I'm not sure'."
)
```

!!! note "Key Question in RAG"
    A crucial aspect of RAG is determining how to select the most relevant information from a large corpus of documents to include in the prompt.

### Addressing LLM Knowledge Gaps: An Example

Consider a scenario where we are interested in Chinese literature, specifically the classic novel "侠客行" (Ode to Gallantry). Suppose we have the text of this novel stored locally.

If we ask a general LLM about specific details, like identifying a main character from a famous scene, it might not have sufficient knowledge or recall. For instance, while some advanced models like DeepSeek might provide a correct answer, others like ChatGPT might struggle without specific context.

This highlights the need for LLMs to access more background information (context). With RAG, we can provide this context directly. Even though we can, in theory, pass large amounts of raw text to an LLM, it's inefficient and doesn't guarantee that the LLM will focus on the most pertinent sections. RAG helps the system (and ultimately the LLM) decide which parts of the text are most relevant to a given question.

### Implementing RAG: Processing Text Data for RAG

Let's walk through an example of preparing text data for a RAG system. We'll use the novel "侠客行" as our source document.

**Assumptions:**
*   The text file `侠客行.txt` is located at `E:\courses\2025S Python\data7\侠客行.txt`.
*   We will store our processed data (vector database) in `E:\courses\2025S Python\data7\chroma_db`.

#### Loading and Splitting Documents

First, we load the document and split it into smaller, manageable chunks.

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings # Updated import

file_path = r"E\courses\2025S Python\data7\侠客行.txt"
loader = TextLoader(file_path, encoding="gb18030")
documents = loader.load()

text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
```

#### Generating Embeddings

Next, we convert these text chunks into numerical representations (embeddings) using an embedding model.

```python
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"
)
# Update to a valid embedding model if needed and ensure API key is configured
```

!!! info "Embeddings"
    Embeddings capture the semantic meaning of text, allowing us to measure similarity between different text passages.

#### Storing Embeddings in a Vector Database

The embedded text chunks are then stored in a vector database. We will use Chroma for this purpose. This database, often referred to as a "vector store," allows for efficient similarity searches.

```python
from langchain_community.vectorstores import Chroma

db_directory = r"E\courses\2025S Python\data7\chroma_db"

# Create and persist the vector store
db = Chroma.from_documents(
    docs,
    embeddings,
    persist_directory=db_directory
)
```

#### Retrieving Relevant Documents

When a user asks a question, we query the vector store to find the most relevant document chunks.

```python
query = "谁是狗杂种" # "Who is Gou Zazhong (a character name)?"

# Retrieve relevant documents based on the query
retriever = db.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": 3, "score_threshold": 0.3},
)
relevant_docs = retriever.invoke(query)
```

#### Constructing the Prompt with Retrieved Context

Finally, we combine the original query with the retrieved relevant text chunks to form the augmented prompt for the LLM.

```python
# Assuming 'query' and 'relevant_docs' are defined as above
combined_input = (
    query
    + "\n\n"
    + "\n\n".join([doc.page_content for doc in relevant_docs])
)

# This combined_input would then be sent to an LLM.
# For example:
# from langchain_openai import ChatOpenAI
# llm = ChatOpenAI(model_name="gpt-3.5-turbo") # Or your preferred model
# response = llm.invoke(combined_input)
# print(response.content)
```

!!! quote "Insight"
    By providing these relevant snippets directly within the prompt, the LLM can generate an answer that is more accurate and contextually grounded.

### Further Steps for RAG

The RAG process described above is a foundational example. There are many ways to extend and improve upon this:

*   **Advanced Text Splitting:** Exploring different strategies for dividing documents into chunks.
*   **Diverse Embedding Models:** Utilizing various models to create text embeddings, potentially tailored to specific domains.
*   **Sophisticated Retrieval Methods:** Implementing more complex techniques to find relevant information, beyond simple similarity search.
*   **Integrating Various LLMs:** Adapting the RAG pipeline to work with different Large Language Models.
*   **Conversational RAG:** Building systems that can maintain context over multiple turns of a conversation using RAG.

!!! info "Further Reading Recommendation"
    For those interested in diving deeper into RAG, exploring resources from experts in the field is highly recommended. (The original notes mention "this guy!", likely referring to a specific content creator or researcher whose materials would be beneficial.)

## A Big Recall: Revisiting Fundamental AI Applications

This section revisits some earlier examples from the course, showcasing basic AI capabilities in different domains.

### Example: Speech Recognition (Whisper)

This example demonstrates how to transcribe an audio file using OpenAI's Whisper model.

```python
import whisper # conda install –c conda-forge ffmpeg openai-whisper
# Ensure ffmpeg is installed: conda install -c conda-forge ffmpeg
# Ensure openai-whisper is installed: pip install openai-whisper

model = whisper.load_model("base")
# Assuming 'audio.m4a' is in the same directory or provide full path
# result = model.transcribe(audio="audio.m4a", fp16=False)
# print(result["text"])
```
!!! note "Code Execution Note"
    The code above sets up the model. To run the transcription, you would uncomment the `transcribe` line and ensure an audio file named `audio.m4a` is available. The output `result` is a dictionary, and `result["text"]` contains the transcribed text.

### Example: Picture Recognition (OCR with EasyOCR)

This example shows how to extract text from an image using EasyOCR.

```python
import easyocr # pip install easyocr

reader = easyocr.Reader(['en'], gpu=False) # Initialize for English, CPU-only
# result = reader.readtext('F:/2025S Python/00 -introduction/good.png', detail=0)
# print(result)
```
!!! note "Code Execution Note"
    To run this, uncomment the `readtext` line and ensure an image file is present at the specified path `'F:/2025S Python/00 -introduction/good.png'`. `detail=0` returns a list of strings. The variable `result` will hold the extracted text.

### Example: Picture Generation (OpenAI DALL-E)

This example illustrates how to generate an image using OpenAI's DALL-E model via its API.

```python
from openai import OpenAI # pip install openai
import os

# Ensure your OpenAI API key is set as an environment variable OPENAI_API_KEY
# OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") 
# client = OpenAI(api_key=OPENAI_API_KEY)

# response = client.images.generate(
#  model="dall-e-2",
#  prompt="driving in Ames in autumn",
#  size="1024x1024",
#  quality="standard",
#  n=1,
# )
# print(response.data[0].url)
```
!!! note "Code Execution and API Key"
    To execute this code:
    1.  Ensure the `openai` library is installed.
    2.  Set your `OPENAI_API_KEY` as an environment variable or directly in the code (less secure).
    3.  Uncomment the lines to create the client and make the API call.
    The output will be a URL pointing to the generated image.

### Learn More?

For further exploration of these topics:

*   Consider watching relevant tutorial videos.
*   Refer to textbooks and official documentation for deeper understanding.
